{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ae7140",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0bcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30610c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load both images, convert to double and to grayscale\n",
    "image1 = cv2.imread('left.jpg')\n",
    "image2 = cv2.imread('right.jpg')\n",
    "\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3110b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Detect feature points in both images\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d820dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute distances between every descriptor in one image and every descriptor in the other image\n",
    "descriptor_distances = distance.cdist(descriptors1, descriptors2, 'sqeuclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5259628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Select putative matches based on the matrix of pairwise descriptor distances\n",
    "# Here, we'll select the top 500 descriptor pairs with the smallest pairwise distances\n",
    "num_matches = 500\n",
    "indices = np.argsort(descriptor_distances, axis=None)[:num_matches]\n",
    "matches = [(index // descriptor_distances.shape[1], index % descriptor_distances.shape[1]) for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86cd271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Implement RANSAC to estimate a homography mapping one image onto the other\n",
    "def ransac(matches, keypoints1, keypoints2, threshold=5, iterations=1000):\n",
    "    best_inliers = []\n",
    "    best_homography = None\n",
    "    best_residual = float('inf')\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Randomly sample 4 matches\n",
    "        sample_indices = np.random.choice(len(matches), 4, replace=False)\n",
    "        sampled_matches = [matches[i] for i in sample_indices]\n",
    "\n",
    "        # Estimate homography\n",
    "        src_pts = np.float32([keypoints1[m[0]].pt for m in sampled_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints2[m[1]].pt for m in sampled_matches]).reshape(-1, 1, 2)\n",
    "        homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, threshold)\n",
    "\n",
    "        # Count inliers\n",
    "        inliers = []\n",
    "        for match in matches:\n",
    "            src_pt = np.float32(keypoints1[match[0]].pt).reshape(-1, 1, 2)\n",
    "            dst_pt = np.float32(keypoints2[match[1]].pt).reshape(-1, 1, 2)\n",
    "            transformed_pt = cv2.perspectiveTransform(src_pt, homography)\n",
    "            error = np.linalg.norm(dst_pt - transformed_pt)\n",
    "            if error < threshold:\n",
    "                inliers.append(match)\n",
    "\n",
    "        # Update best model if necessary\n",
    "        if len(inliers) > len(best_inliers):\n",
    "            best_inliers = inliers\n",
    "            best_homography = homography\n",
    "            best_residual = np.mean([np.linalg.norm(np.float32(keypoints2[m[1]].pt).reshape(-1, 1, 2) - cv2.perspectiveTransform(np.float32(keypoints1[m[0]].pt).reshape(-1, 1, 2), best_homography)) for m in best_inliers])\n",
    "\n",
    "    return best_homography, best_inliers, best_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65bf479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "homography, inliers, residual = ransac(matches, keypoints1, keypoints2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "849f4daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inliers: 77\n",
      "Average residual: 2.4247596\n"
     ]
    }
   ],
   "source": [
    "# Report the number of inliers and the average residual for the inliers\n",
    "print(\"Number of inliers:\", len(inliers))\n",
    "print(\"Average residual:\", residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecdf4218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the locations of inlier matches in both images\n",
    "result = cv2.drawMatches(image1, keypoints1, image2, keypoints2, [cv2.DMatch(i, i, 0) for i in range(len(inliers))], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imwrite(\"Matches.jpg\", result)\n",
    "#cv2.imshow(\"Matches\", result)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824402e",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c142b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "#import numpy as np\n",
    "#from scipy.spatial import distance\n",
    "#from skimage.transform import ProjectiveTransform, warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07464880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "#image1 = cv2.imread('left.jpg')\n",
    "#image2 = cv2.imread('right.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e91807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to grayscale\n",
    "#gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "#gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8354b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect feature points and compute descriptors\n",
    "#sift = cv2.xfeatures2d.SIFT_create()\n",
    "#keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "#keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89e9ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise descriptor distances\n",
    "#descriptor_distances = distance.cdist(descriptors1, descriptors2, 'sqeuclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddb476a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select putative matches based on the matrix of pairwise descriptor distances\n",
    "#num_matches = 500\n",
    "#indices = np.argsort(descriptor_distances, axis=None)[:num_matches]\n",
    "#matches = [(index // descriptor_distances.shape[1], index % descriptor_distances.shape[1]) for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "637771e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Implement RANSAC to estimate a homography\n",
    "def ransac(matches, keypoints1, keypoints2, threshold=5, iterations=1000):\n",
    "    best_inliers = []\n",
    "    best_homography = None\n",
    "    best_residual = float('inf')\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Randomly sample 4 matches\n",
    "        sample_indices = np.random.choice(len(matches), 4, replace=False)\n",
    "        sampled_matches = [matches[i] for i in sample_indices]\n",
    "\n",
    "        # Estimate homography\n",
    "        src_pts = np.float32([keypoints1[m[0]].pt for m in sampled_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints2[m[1]].pt for m in sampled_matches]).reshape(-1, 1, 2)\n",
    "        homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, threshold)\n",
    "\n",
    "        # Count inliers\n",
    "        inliers = []\n",
    "        for match in matches:\n",
    "            src_pt = np.float32(keypoints1[match[0]].pt).reshape(-1, 1, 2)\n",
    "            dst_pt = np.float32(keypoints2[match[1]].pt).reshape(-1, 1, 2)\n",
    "            transformed_pt = cv2.perspectiveTransform(src_pt, homography)\n",
    "            error = np.linalg.norm(dst_pt - transformed_pt)\n",
    "            if error < threshold:\n",
    "                inliers.append(match)\n",
    "\n",
    "        # Update best model if necessary\n",
    "        if len(inliers) > len(best_inliers):\n",
    "            best_inliers = inliers\n",
    "            best_homography = homography\n",
    "            best_residual = np.mean([np.linalg.norm(np.float32(keypoints2[m[1]].pt).reshape(-1, 1, 2) - cv2.perspectiveTransform(np.float32(keypoints1[m[0]].pt).reshape(-1, 1, 2), best_homography)) for m in best_inliers])\n",
    "\n",
    "    return best_homography, best_inliers, best_residual\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ce8ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Run RANSAC\n",
    "homography, inliers, residual = ransac(matches, keypoints1, keypoints2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e5bebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warp image1 onto image2 using the estimated homography\n",
    "h, w = image2.shape[:2]\n",
    "warped_image = cv2.warpPerspective(image1, homography, (w, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "035d43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite the two images by averaging pixel values\n",
    "# You can also use other blending techniques here\n",
    "composite_image = (warped_image + image2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "656616d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or display the resulting composite image\n",
    "cv2.imwrite(\"composite_image.jpg\", composite_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
